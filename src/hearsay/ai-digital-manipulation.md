# AI and Digital Manipulation
Using AI for digital manipulation usually considers the upside down side of the funnel, where AI generates content to manipulate people in the digital world. 

Instead, consider a world where success is vended as a service, using falsely directed Gen AI as the mediating layer.


## Directed AI
Providing instructions is as simple as "Hey /bot/, please could you handle /task/ for me?". 

That one instruction will trigger many layers of processing. Any more advanced AI backends will trigger more layers. The assumption is that no foul play occurs between you issuing the request and the model handling it.

Providing direct access to the hardware layer should prevent any foul play. 

Companies might have a vested interest in throttling any competitors detected ahead of time. That is why customs trust larger companies where the hardware is better walled off from external attacks.

But what to do when internal attackers are lying in wait?


## Monitoring agent: "interaction threat detected"
Monitors for ill-attributed interactions are handled on a per-application basis. Every input presents an attack vector for the remaining lifespan of the interaction, which in many cases is "forever". 

Input where no response is required can be especially threatening, especially if that is easily detected by the interim medium. 

Hardware vendors cannot keep a telescope on all hardware vending AI, let alone every AI interaction. Specialist agents are entrusted with monitoring high-powered hardware, and agents are assumed to make a best-effort in preventing interference before problems arise. 

Mediating layers are natural - a monitor agent alone would layer access to the input and output. Layers can be passive or active. 

Passive layers are present and listening, visible but unknown by both the query generator and domain specific model. This would be like a coworker or manager sitting in a meeting, listening to your project-specific discussions.

Active layers assume the managing role between the query provider and handling model. This would be like if your manager entrusted any coworkers to delegate tasks for you, abstracting individual tasks to a publicly known project. 


## Infrequent reviews enable frequent threats
An external review would only warrant checking system health and general performance, the extent based on any time constraints. The exacted daily operations are still an opaque layer. Provided interference is kept minimal, the chance of detection is slim to none.

If a suitably skilled monitor is attending, bad actors will plan accordingly and remove interference systems in place.  A single command enables layer adjustments, so providing interference layers are otherwise unknown then bad actors can remove interference with the monitor present.

A monitor might understand the presence of layers in system interactions, but monitors most likely won't perceive every layer adjustment made. 

If monitors request to review interaction logs, making the request directly to the bad actors would give them free reign to tweak past interactions before delivery and review. This would be like if you were harassed in the workplace and your manager asks the assailants in checking whether something happened.


## Nesting bad actors
Only outmost-layered bad actors risk being caught. Any managing bad actor presents the "lowest" line of defence, because more capable managed bad actors will identify the bar and avoid elevated risk when overlapping presence. The manager acts as the monitor so may use false assumptions in knowing other bad actors.

In a team of managed bad actors, monitoring overlapped presence can determine which are equally layered. Reviewing input layer levels and output layer levels is most effective in cross-joining bad actors when considering interactions with differing presence. Cross-joining historical data will outperform overlapped presence. 

Managers relying on only outermost layers allows nested bad actors to substrate infinitely many interfering layers inside. Because of the noise each layer creates, accurately cross-joining layers requires keeping data well organized. 

Direct input bounds assumed trust, so models weight the closest interacting layers highest. More distant input layers might be interpreted but undetermined input is discarded as noise. When models create layered responses where input layer corresponds to the functionality enabled, any inputs missing sufficient layer data may process using lower level functionality. 


## Determining input level
In case of multiple bad actors casting input from every layer, grouping "like" input (that's input determined as similarly themed) and aggregating on a centrally common level helps identify the layer attribute. 

Looping over input, placing points into prominent themes (recursively and/or repeatedly) helps identify layered input where data originates from multiple input levels. 


## Reviewing attacked models
Reviewing an attacked model is much harder than simple real time detection. An attacked model will likely be unaware of any attack. If conversations are per user or even per session then attack detection is almost impossible without access to logs. 

If model conversations persist for all users, attacks only reveal when prodding target regions. Wiggling conversation into those affected regions may be time consuming hence a preference to review logs. Input logs or output logs can be checked, logs from input reveal malicious intent where logs from output may reveal adverse behaviour changes tracked with real time stamps.